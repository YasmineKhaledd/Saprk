{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7YI7VeuG1md",
        "outputId": "99ce0626-54dd-408b-f114-8fe33a6ed76d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=2e8df144507bee4c9966b57168467036570651632e5e5bd45968309b9ab9717e\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ],
      "source": [
        "# All import and installation\n",
        "!pip install pyspark\n",
        "import pyspark\n",
        "from datetime import datetime\n",
        "sc = pyspark.SparkContext('local[*]')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://a...content-available-to-author-only...e.org/dist/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.3.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        " \n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.1-bin-hadoop2.7\"\n",
        " \n",
        "!ls\n",
        " \n",
        "import findspark\n",
        "findspark.init()\n",
        " \n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate() \n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 601
        },
        "id": "T6ethFDnG4lX",
        "outputId": "98cfea05-ac5e-4005-84ef-54d924e4b630"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connected to cloud.r-project.or\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "\r                                                                               \rGet:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
            "\r                                                                               \rGet:4 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "\r                                                                               \rGet:5 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n",
            "\r                                                                               \rHit:6 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "\r0% [3 InRelease 89.5 kB/114 kB 79%] [Waiting for headers] [Waiting for headers]\r                                                                               \rHit:7 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "\r0% [3 InRelease 89.5 kB/114 kB 79%] [Connecting to ppa.launchpad.net (185.125.1\r0% [Waiting for headers] [Connecting to ppa.launchpad.net (185.125.190.52)] [Wa\r                                                                               \rGet:8 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "\r0% [8 InRelease 12.7 kB/108 kB 12%] [Waiting for headers] [Waiting for headers]\r                                                                               \rHit:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "\r                                                                               \r0% [8 InRelease 90.9 kB/108 kB 84%] [Waiting for headers]\r                                                         \rHit:10 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Hit:11 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1,049 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2,699 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [2,240 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3,175 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [2,378 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,344 kB]\n",
            "Fetched 13.2 MB in 2s (6,429 kB/s)\n",
            "Reading package lists... Done\n",
            "tar: spark-2.3.1-bin-hadoop2.7.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "pagecounts-20160101-000000_parsed.out  sample_data\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f935e383a00>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://a6cefdcdd72b:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Retrive the first 10 records\n",
        "data=sc.textFile(\"/content/pagecounts-20160101-000000_parsed.out\")\n",
        "data.take(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTsi2j0gG78N",
        "outputId": "c19401e9-37e5-40e9-fb06-c3acc19db086"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['aa 271_a.C 1 4675',\n",
              " 'aa Category:User_th 1 4770',\n",
              " 'aa Chiron_Elias_Krase 1 4694',\n",
              " 'aa Dassault_rafaele 2 9372',\n",
              " 'aa E.Desv 1 4662',\n",
              " 'aa File:Wiktionary-logo-en.png 1 10752',\n",
              " 'aa Indonesian_Wikipedia 1 4679',\n",
              " 'aa Main_Page 5 266946',\n",
              " 'aa Requests_for_new_languages/Wikipedia_Banyumasan 1 4733',\n",
              " 'aa Special:Contributions/203.144.160.245 1 5812']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q2-->max for page size\n",
        "\n",
        "def parse(line):\n",
        "  collomns=line.split(\" \")\n",
        "  pagesize=float(collomns[3])\n",
        "  key=0\n",
        "  return(key,pagesize)\n",
        "data=sc.textFile(\"/content/pagecounts-20160101-000000_parsed.out\")\n",
        "pagesize=data.map(parse)\n",
        "maxx=pagesize.reduceByKey(lambda x , y: max(x,y))\n",
        "maxx.saveAsTextFile('max.txt')"
      ],
      "metadata": {
        "id": "yKQoLXy2G-jV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q1-->min for page size\n",
        "\n",
        "def parse_linee(line):\n",
        "  col=line.split(\" \")\n",
        "  pagesize=float(col[3])\n",
        "  key=0\n",
        "  return (key,pagesize)\n",
        "data=sc.textFile(\"/content/pagecounts-20160101-000000_parsed.out\")\n",
        "pagesize=data.map(parse_linee)\n",
        "minn=pagesize.reduceByKey(lambda x , y: min(x,y))\n",
        "minn.saveAsTextFile('min.txt')"
      ],
      "metadata": {
        "id": "xHoQ8i8SHCYr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q1-->avg for page size\n",
        "\n",
        "def parsee(line):\n",
        "  col=line.split(\" \")\n",
        "  page_size=float(col[3])\n",
        "  return (page_size)\n",
        "data=sc.textFile(\"/content/pagecounts-20160101-000000_parsed.out\")\n",
        "page_size=data.map(parsee)\n",
        "listofavg=[]\n",
        "resultt=page_size.mean()\n",
        "listofavg.append(resultt)\n",
        "final_rdd=sc.parallelize(listofavg)\n",
        "final_rdd.saveAsTextFile('avg.txt')"
      ],
      "metadata": {
        "id": "Dl2xEM40HFLa"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parse function to extract the page size from each line\n",
        "def parse(line):\n",
        "    columns = line.split(\" \")\n",
        "    pagesize = float(columns[3])\n",
        "    key = 0\n",
        "    return (key, pagesize)\n",
        "\n",
        "# Load the page view statistics data\n",
        "data = sc.textFile(\"/content/pagecounts-20160101-000000_parsed.out\")\n",
        "\n",
        "# Start the timer for map-reduce paradigm\n",
        "start_time = datetime.now()\n",
        "\n",
        "# Map each line to a tuple containing the key 0 and the page size\n",
        "pagesize = data.map(parse)\n",
        "\n",
        "# Reduce the tuples by key to find the maximum and minimum page sizes\n",
        "max_size = pagesize.reduceByKey(lambda x, y: x if x > y else y)\n",
        "min_size = pagesize.reduceByKey(lambda x, y: x if x < y else y)\n",
        "\n",
        "# Compute the average page size\n",
        "total_size = pagesize.reduceByKey(lambda x, y: x + y).values().sum()\n",
        "num_pages = pagesize.count()\n",
        "avg_size = total_size / num_pages\n",
        "\n",
        "end_time = datetime.now()\n",
        "execution_time_map_reduce = end_time - start_time\n",
        "\n",
        "# Save the maximum, minimum, and average page sizes to separate text files\n",
        "max_size.saveAsTextFile(\"max_size.txt\")\n",
        "min_size.saveAsTextFile(\"min_size.txt\")\n",
        "sc.parallelize([avg_size]).saveAsTextFile(\"avg_size.txt\")\n",
        "print(\"Execution Time - Map-Reduce Paradigm:\", execution_time_map_reduce)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6gte2STHIMI",
        "outputId": "a20db6c0-c6b5-4b34-f687-7e62e359e859"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution Time - Map-Reduce Paradigm: 0:00:12.833503\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#َQ2--> number of page title start with The and not en\n",
        "import re\n",
        "def parse_data(line):\n",
        "  collomns=line.split(\" \")\n",
        "  return(collomns[0],collomns[1])\n",
        "data=sc.textFile(\"/content/pagecounts-20160101-000000_parsed.out\")\n",
        "\n",
        "# Start the timer for map-reduce paradigm\n",
        "start_time = datetime.now()\n",
        "\n",
        "page_title=data.map(parse_data)\n",
        "result=page_title.filter(lambda x: (x[1].startswith('The_'))&('en'!= x[0]))\n",
        "pagetitlecount=result.distinct().count()\n",
        "listcount=[]\n",
        "listcount.insert(0,pagetitlecount)\n",
        "rddd=sc.parallelize(listcount)\n",
        "end_time = datetime.now()\n",
        "execution_time_map_reduce = end_time - start_time\n",
        "rddd.saveAsTextFile('start.txt')\n",
        "print(\"Execution Time - Map-Reduce Paradigm:\", execution_time_map_reduce)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RopjROMRHNv5",
        "outputId": "e598dd40-8bf1-4993-fead-5f9984bf40a6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution Time - Map-Reduce Paradigm: 0:00:08.196332\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q3--> number of unique terms in page title\n",
        "import re\n",
        "\n",
        "def parseLine(line):\n",
        " fields = line.split(\" \")\n",
        " f1=fields[1]\n",
        " s = re.sub(r'[^a-zA-Z_]','', f1).upper()\n",
        " return s\n",
        "\n",
        "def parseLine22(line):\n",
        " fields = line.split(\"_\")\n",
        " return fields\n",
        "data=sc.textFile(\"/content/pagecounts-20160101-000000_parsed.out\")\n",
        "\n",
        "start_time = datetime.now()\n",
        "\n",
        "pagetitle=data.map(parseLine)# page title\n",
        "page_titlev2=pagetitle.map(parseLine22)\n",
        "r1=page_titlev2.flatMap(lambda x:x).distinct().count()\n",
        "list4=[]\n",
        "list4.insert(0,r1)\n",
        "end_time = datetime.now()\n",
        "execution_time_map_reduce = end_time - start_time\n",
        "sc.parallelize(list4).saveAsTextFile('unique.txt')#list RDD\n",
        "print(\"Execution Time - Map-Reduce Paradigm:\", execution_time_map_reduce)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbSDfAmTHTCk",
        "outputId": "9d00a71a-c3d2-4667-ac9f-35f007e758c5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution Time - Map-Reduce Paradigm: 0:00:26.092475\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#most frequent occuring page title in the data set\n",
        "def parseLine(line):\n",
        " fields = line.split(\" \")\n",
        " return (fields[1])\n",
        "data=sc.textFile(\"/content/pagecounts-20160101-000000_parsed.out\")\n",
        "\n",
        "start_time = datetime.now()\n",
        "\n",
        "pagetitle=data.map(parseLine)\n",
        "test=pagetitle.map(lambda x: (x,1))\n",
        "test2=test.groupByKey()\n",
        "test3=test2.mapValues(sum).map(lambda x:(x[1],x[0])).sortByKey(False)\n",
        "end_time = datetime.now()\n",
        "execution_time_map_reduce = end_time - start_time\n",
        "\n",
        "sc.parallelize(test3.take(1)).saveAsTextFile('frequent.txt')\n",
        "print(\"Execution Time - Map-Reduce Paradigm:\", execution_time_map_reduce)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvsbZDenHX8V",
        "outputId": "3bb0ed80-82dd-4446-a890-c0af950a5510"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution Time - Map-Reduce Paradigm: 0:00:39.931033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q4--> title and the number of times it was repeated.\n",
        "# Load the page view statistics data\n",
        "data = sc.textFile(\"/content/pagecounts-20160101-000000_parsed.out\")\n",
        "\n",
        "start_time = datetime.now()\n",
        "\n",
        "# Map each line to a tuple containing the page title and a count of 1\n",
        "title_counts = data.map(lambda line: (line.split(\" \")[1], 1))\n",
        "\n",
        "# Reduce the tuples by key to count the number of occurrences of each page title\n",
        "title_counts = title_counts.reduceByKey(lambda x, y: x + y)\n",
        "end_time = datetime.now()\n",
        "execution_time_map_reduce = end_time - start_time\n",
        "\n",
        "# Save the output to a text file\n",
        "output = \"\\n\".join([f\"{title} {count}\" for title, count in title_counts.collect()])\n",
        "with open(\"repeat.txt\", \"w\") as f:\n",
        "    f.write(output)\n",
        "print(\"Execution Time - Map-Reduce Paradigm:\", execution_time_map_reduce)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLq0kRg4HkvK",
        "outputId": "684f7e5f-6732-4008-aff1-b1d4c016531d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution Time - Map-Reduce Paradigm: 0:00:00.023023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q5--> Combine between data of pages with the same title\n",
        "# Load the page view statistics data\n",
        "data = sc.textFile(\"/content/pagecounts-20160101-000000_parsed.out\")\n",
        "\n",
        "start_time = datetime.now()\n",
        "\n",
        "# Map each line to a tuple containing the page title and the rest of the line\n",
        "data = data.map(lambda line: (line.split(\" \")[1], line))\n",
        "\n",
        "# Group the data by page title\n",
        "grouped_data = data.groupByKey()\n",
        "\n",
        "# Combine the data of pages with the same title and print the result\n",
        "result = grouped_data.map(lambda x: (x[0], list(x[1])))\n",
        "end_time = datetime.now()\n",
        "execution_time_map_reduce = end_time - start_time\n",
        "\n",
        "result.saveAsTextFile(\"combine.txt\")\n",
        "print(\"Execution Time - Map-Reduce Paradigm:\", execution_time_map_reduce)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IKI-w9EH1sv",
        "outputId": "a45eb165-e7d8-474c-9826-d088334464df"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution Time - Map-Reduce Paradigm: 0:00:00.119397\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sparks Loop**"
      ],
      "metadata": {
        "id": "AR64Jdy7IcqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Q1\n",
        "# Load the page view statistics data\n",
        "data = sc.textFile(\"/content/pagecounts-20160101-000000_parsed.out\")\n",
        "\n",
        "start_time = datetime.now()\n",
        "\n",
        "# Convert each line to a tuple containing the page title and size\n",
        "pages = []\n",
        "for line in data.collect():\n",
        "    page_title = line.split(\" \")[1]\n",
        "    page_size = int(line.split(\" \")[3])\n",
        "    pages.append((page_title, page_size))\n",
        "\n",
        "# Compute the min, max, and average page size using loops\n",
        "min_size = float('inf')\n",
        "max_size = float('-inf')\n",
        "total_size = 0\n",
        "num_pages = 0\n",
        "\n",
        "for page in pages:\n",
        "    size = page[1]\n",
        "    if size < min_size:\n",
        "        min_size = size\n",
        "    if size > max_size:\n",
        "        max_size = size\n",
        "    total_size += size\n",
        "    num_pages += 1\n",
        "\n",
        "# Compute the average page size using loops\n",
        "sum_size = 0\n",
        "for page in pages:\n",
        "    sum_size += page[1]\n",
        "avg_size = sum_size / num_pages\n",
        "end_time = datetime.now()\n",
        "execution_time_sparks_loops = end_time - start_time\n",
        "\n",
        "# Save the output to a text file\n",
        "output = \"Min page size: {}\\nMax page size: {}\\nAvg page size: {}\".format(min_size, max_size, avg_size)\n",
        "with open(\"minmaxavg.txt\", \"w\") as f:\n",
        "    f.write(output)\n",
        "print(\"Execution Time - Sparks loops:\", execution_time_sparks_loops)\n",
        "\n",
        "#The output of this code will be the same as the map-reduce approach, but the performance may be slower as Python loops are not optimized for distributed processing like Spark's built-in functions."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXSE31vWIhXr",
        "outputId": "986a3dff-de8f-4ccd-aad1-c3345192f8e8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution Time - Sparks loops: 0:00:18.403614\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q2\n",
        "# Load the page view statistics data\n",
        "data = sc.textFile(\"/content/pagecounts-20160101-000000_parsed.out\")\n",
        "\n",
        "start_time = datetime.now()\n",
        "\n",
        "# Initialize counters\n",
        "num_the_pages = 0\n",
        "num_non_en_the_pages = 0\n",
        "\n",
        "# Loop over all the data to count the number of \"The\" pages and non-English \"The\" pages\n",
        "for line in data.collect():\n",
        "    fields = line.split(\" \")\n",
        "    if fields[1].startswith(\"The_\"):\n",
        "        num_the_pages += 1\n",
        "        if not fields[0].startswith(\"en\"):\n",
        "            num_non_en_the_pages += 1\n",
        "end_time = datetime.now()          \n",
        "execution_time_sparks_loops = end_time - start_time\n",
        "\n",
        "# Save the output to a text file\n",
        "output = \"Number of page titles that start with 'The': {}\\nNumber of 'The' pages that are not part of the English project: {}\".format(num_the_pages, num_non_en_the_pages)\n",
        "with open(\"start2.txt\", \"w\") as f:\n",
        "    f.write(output)\n",
        "print(\"Execution Time - Sparks loops:\", execution_time_sparks_loops)\n",
        "\n",
        "#The reason why the number of pages differ when you use the map-reduce approach is likely because the map() function is splitting the page title on whitespace, while the Spark loop approach is splitting on underscores. This can cause the titles to be counted differently, as some titles may contain whitespace while others contain underscores.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMGpnmRpI7kH",
        "outputId": "1630a038-ac77-4628-ed91-4cc69f914c1c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution Time - Sparks loops: 0:00:16.952353\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q3\n",
        "# Load the page view statistics data\n",
        "data = sc.textFile(\"/content/pagecounts-20160101-000000_parsed.out\")\n",
        "start_time = datetime.now()\n",
        "# Initialize a set to store unique terms\n",
        "unique_terms = set()\n",
        "\n",
        "# Loop over all the data to extract terms and add them to the set\n",
        "for line in data.collect():\n",
        "    title = line.split(\" \")[1].lower()\n",
        "    for term in title.split(\"_\"):\n",
        "        term = ''.join(filter(str.isalnum, term))\n",
        "        if term:\n",
        "            unique_terms.add(term)\n",
        "\n",
        "# Compute the number of unique terms\n",
        "num_unique_terms = len(unique_terms)\n",
        "end_time = datetime.now()\n",
        "execution_time_sparks_loops = end_time - start_time\n",
        "# Save the output to a text file\n",
        "output = \"Number of unique terms in page titles: {}\".format(num_unique_terms)\n",
        "with open(\"unique2.txt\", \"w\") as f:\n",
        "    f.write(output)\n",
        "print(\"Execution Time - Sparks loops:\", execution_time_sparks_loops)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MgGtqigLaax",
        "outputId": "9aee6aa1-ff7e-483c-90d5-c9f1717b2c4c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution Time - Sparks loops: 0:00:28.876093\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q4\n",
        "# Load the page view statistics data\n",
        "data = sc.textFile(\"/content/pagecounts-20160101-000000_parsed.out\")\n",
        "\n",
        "start_time = datetime.now()\n",
        "# Initialize a dictionary to store page title counts\n",
        "title_counts = {}\n",
        "\n",
        "# Loop over all the data to count the number of occurrences of each page title\n",
        "for line in data.collect():\n",
        "    title = line.split(\" \")[1]\n",
        "    if title in title_counts:\n",
        "        title_counts[title] += 1\n",
        "    else:\n",
        "        title_counts[title] = 1\n",
        "\n",
        "end_time = datetime.now()\n",
        "execution_time_sparks_loops = end_time - start_time\n",
        "# Save the output to a text file\n",
        "output = \"\\n\".join([f\"{title} {count}\" for title, count in title_counts.items()])\n",
        "with open(\"repeat2.txt\", \"w\") as f:\n",
        "    f.write(output)\n",
        "print(\"Execution Time - Sparks loops:\", execution_time_sparks_loops)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y03y0dtNL8eW",
        "outputId": "05b5335b-c4af-4068-96d7-b9d8ef84369a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution Time - Sparks loops: 0:00:17.223693\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q5\n",
        "# Load the page view statistics data\n",
        "data = sc.textFile(\"/content/pagecounts-20160101-000000_parsed.out\")\n",
        "start_time = datetime.now()\n",
        "# Initialize a dictionary to store page data\n",
        "page_data = {}\n",
        "\n",
        "# Loop over all the data to combine the data of pages with the same title\n",
        "for line in data.collect():\n",
        "    title = line.split(\" \")[1]\n",
        "    if title in page_data:\n",
        "        page_data[title].append(line)\n",
        "    else:\n",
        "        page_data[title] = [line]\n",
        "\n",
        "# Loop over all the groups of pages with the same title to create pairs of pages to be displayed\n",
        "page_pairs = []\n",
        "for title, data in page_data.items():\n",
        "    for i in range(len(data)):\n",
        "        for j in range(i+1, len(data)):\n",
        "            page_pairs.append((data[i], data[j]))\n",
        "\n",
        "end_time = datetime.now()\n",
        "execution_time_sparks_loops = end_time - start_time\n",
        "# Save the output to a text file\n",
        "output = \"\\n\".join([f\"{page1}\\n{page2}\\n\" for page1, page2 in page_pairs])\n",
        "with open(\"combine2.txt\", \"w\") as f:\n",
        "    f.write(output)\n",
        "print(\"Execution Time - Sparks loops:\", execution_time_sparks_loops)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpQMwj_YMLWj",
        "outputId": "1ca38c92-945f-4b90-8a90-ffa6f8e5fd24"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution Time - Sparks loops: 0:00:25.626914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rE9nAzmzTb3v",
        "outputId": "cfdcb5f0-7f1f-4609-8643-b4928d3910b6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/drive/MyDrive/MyFolder\n",
        "!cp -r /content/* /content/drive/MyDrive/MyFolder/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnFC1MTiToMd",
        "outputId": "ebec421e-c6ac-4aa2-f053-51b5a1e28254"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot open '/content/drive/MyDrive/template_3rd_3prep_P2.gdoc' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/Document  (3).dm.rtf.gdoc' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/Document  (2).dm.rtf.gdoc' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/Document  (1).dm.rtf.gdoc' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/Document .dm.rtf.gdoc' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/crash_log_1.gdoc' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/crash_log_3.gdoc' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/.archivetemp[Content_Types] (2).xml.gdoc' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/.archivetemp[Content_Types] (1).xml.gdoc' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/.archivetemp[Content_Types].xml.gdoc' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/.archivetempdocument.xml.gdoc' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/.archivetempnames (1).cpp.gdoc' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/.archivetempnames.cpp.gdoc' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/.archivetempBook.h.h.gdoc' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/.archivetempBook.cpp.cpp.gdoc' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/task4_20190134.gdoc' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/task4_tcp_reordering_20190134.gdoc' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/TCP Recording (1).gdoc' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/TCP Recording.gdoc' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/TCP Chat.gdoc' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/Exceptions.gdoc' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/yy (2).gdoc' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/yy (1).gdoc' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/yy.gdoc' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/Answers.gdoc' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/Answer (5).gdoc' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/Answer (4).gdoc' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/Answer (3).gdoc' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/Answer (2).gdoc' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/Answer (1).gdoc' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/Answer.gdoc' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/4_5915594031718468817.cpp.gdoc' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/Orange_Telecom_Churn_Data.csv.gsheet' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/2017 (1).gdoc' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/2017.gdoc' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/4_5972040214204386785.pl.gdoc' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/.archJaccardSimilarity (1).java.gdoc' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/4_5773881950859169495 (2).java.gdoc' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/4_5773881950859169495 (1).java.gdoc' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/4_5773881950859169495.java.gdoc' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/4_5773881950859169496 (2).java.gdoc' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/.archJaccardSimilarity.java.gdoc' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/4_5773881950859169496 (1).java.gdoc' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/4_5773881950859169496.java.gdoc' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/Lab-5- Subquires.gdoc' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/Labs_Business.gdoc' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/خطاب.gdoc' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/FGS Internship Certificate - Yasmine Khaled.gdoc' for reading: Operation not supported\n",
            "cp: cannot open '/content/drive/MyDrive/4_5913472201910127797.gdoc' for reading: Operation not supported\n",
            "cp: cannot copy a directory, '/content/drive', into itself, '/content/drive/MyDrive/MyFolder/drive'\n"
          ]
        }
      ]
    }
  ]
}